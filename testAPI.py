from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
# --- ì¶”ê°€ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ ---
from fastapi.responses import FileResponse
from fastapi.staticfiles import StaticFiles
# -------------------------
import os
import uuid
from dotenv import load_dotenv

# --- Google Gemini ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½ ---
from langchain_community.document_loaders import PyPDFLoader
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

os.getenv("GOOGLE_API_KEY")

app = FastAPI()

# --- ğŸ”½ [ì¶”ê°€ëœ ì½”ë“œ ì‹œì‘] ğŸ”½ ---
# ì‚¬ìš©ìê°€ ì›¹ì‚¬ì´íŠ¸ì˜ ë£¨íŠ¸ ì£¼ì†Œ('/')ë¡œ ì ‘ì†í–ˆì„ ë•Œ
# 'static' í´ë”ì— ìˆëŠ” 'index.html' íŒŒì¼ì„ ì‘ë‹µìœ¼ë¡œ ë³´ë‚´ì¤ë‹ˆë‹¤.
# ì´ë ‡ê²Œ í•˜ë©´ í”„ë¡ íŠ¸ì—”ë“œì™€ ë°±ì—”ë“œê°€ í•˜ë‚˜ì˜ ì£¼ì†Œì—ì„œ í•¨ê»˜ ë™ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
@app.get("/", include_in_schema=False)
async def read_index():
    return FileResponse(os.path.join("static", "index.html"))
# --- ğŸ”¼ [ì¶”ê°€ëœ ì½”ë“œ ë] ğŸ”¼ ---


origins = [
    "http://localhost",
    "http://localhost:8080",
    "http://127.0.0.1",
    "http://127.0.0.1:8080",
    "http://localhost:63342",
    "null"
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

UPLOAD_DIRECTORY = "/tmp/pdf_uploads"
if not os.path.exists(UPLOAD_DIRECTORY):
    os.makedirs(UPLOAD_DIRECTORY)

# --- OpenAI ëŒ€ì‹  Google Gemini ëª¨ë¸ì„ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½ ---
try:
    # Google API í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸
    if not os.getenv("GOOGLE_API_KEY"):
        raise ValueError("GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

    # Google Gemini ì„ë² ë”© ëª¨ë¸ê³¼ LLM ì´ˆê¸°í™”
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-pro-latest", temperature=0, convert_system_message_to_human=True)

except Exception as e:
    print(f"Google Gemini ëª¨ë¸ì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”. ì˜¤ë¥˜: {e}")
    embeddings = None
    llm = None
# ----------------------------------------------------


QUESTION_PROMPT_TEMPLATE = """
ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ í•µì‹¬ì ì¸ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì»¨í…ìŠ¤íŠ¸ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ëŒë“¤ì´ ê¶ê¸ˆí•´í•  ë§Œí•œ ì¤‘ìš”í•œ ì§ˆë¬¸ì„ **í•œ ê°œë§Œ** ë§Œë“œì„¸ìš”.
ì§ˆë¬¸ì€ ê°„ê²°í•˜ê³  ëª…í™•í•´ì•¼ í•©ë‹ˆë‹¤.

--- ì»¨í…ìŠ¤íŠ¸ ---
{context}
-----------------

ìƒì„±ëœ ì§ˆë¬¸:
"""


@app.post("/generate-questions-from-pdf/")
async def generate_questions_from_pdf(file: UploadFile = File(...)):
    # --- Google Gemini ì„œë¹„ìŠ¤ ì¤€ë¹„ ìƒíƒœë¥¼ í™•ì¸í•˜ë„ë¡ ë³€ê²½ ---
    if not llm or not embeddings:
        raise HTTPException(status_code=503, detail="Google Gemini ì„œë¹„ìŠ¤ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ ë¡œê·¸ë‚˜ API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    unique_id = uuid.uuid4().hex
    file_path = os.path.join(UPLOAD_DIRECTORY, f"{unique_id}_{file.filename}")

    try:
        with open(file_path, "wb") as buffer:
            buffer.write(await file.read())

        loader = PyPDFLoader(file_path)
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        split_docs = text_splitter.split_documents(documents)

        if not split_docs:
            raise HTTPException(status_code=400, detail="PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ê±°ë‚˜ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        if os.path.exists(file_path):
            os.remove(file_path)

    question_generation_prompt = ChatPromptTemplate.from_template(QUESTION_PROMPT_TEMPLATE)
    question_chain = question_generation_prompt | llm | StrOutputParser()

    generated_questions = []
    docs_to_process = split_docs[:5]

    for doc in docs_to_process:
        generated_question = question_chain.invoke({"context": doc.page_content})
        generated_questions.append(generated_question.strip())

    return {
        "message": f"ì´ {len(split_docs)}ê°œì˜ ì¡°ê° ì¤‘ {len(docs_to_process)}ê°œì— ëŒ€í•œ ì§ˆë¬¸ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
        "questions": generated_questions

    }

